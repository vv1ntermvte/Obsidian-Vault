The second framework that will be examined is Paths-over-Graph (PoG), which introduces a methodology designed to enhance Large Language Model (LLM) reasoning by, similar to LightPROF,focusing on retrieving and processing interconnected reasoning paths that logically link topic entities. This method specifically targets the difficulties associated with multi-hop reasoning, multi-entity questions, and the under-utilization of graph structural information. The architecture of PoG is composed of four distinct, sequential phases: Initialization, Exploration, Path Pruning, and Question Answering.

Initialization. The process commences with the Initialization phase, which establishes the necessary foundation for efficient graph exploration through two key sub-processes: Question Subgraph Detection and Question Analysis. Initially, the framework identifies topic entities within the user's input question and aligns them with corresponding entities in the Knowledge Graph (KG) utilizing BERT-based similarity matching. Subsequently, a "question subgraph" (Gq​) is constructed by expanding these topic entities to include their neighbors up to a user-defined maximum depth (Dmax​). This subgraph encapsulates all potentially relevant entities and relations. To manage the computational costs and potential noise inherent in large KGs, this subgraph is immediately pruned. Techniques such as node and relation clustering are employed to compress the graph by aggregating similar elements into supernodes. Furthermore, a graph reduction technique using bidirectional BFS is applied to identify and retain only those paths that successfully connect the topic entities, effectively discarding irrelevant branches and unconnected nodes.

Simultaneously, the Question Analysis phase leverages the capabilities of the LLM to decompose the complex input question into simpler, manageable sub-questions. This decomposition not only guides the subsequent reasoning process but also serves to mitigate hallucination by focusing the model's attention. Crucially, the LLM generates a "thinking indicator", a strategic plan that outlines the sequence of entities and predicts the likely depth of the answer (Dpredict​) within the graph structure. This predictive capability allows the subsequent exploration phase to begin at an intelligent, estimated depth rather than searching blindly from scratch, thereby optimizing search efficiency.

Exploration. Following initialization, in the Exploration phase the pruned subgraph Gq is actively traversed to discover faithful reasoning paths. This process is executed through three dynamic, hierarchical stages to ensure comprehensive coverage:

1. Topic Entity Path Exploration: Guided by the LLM's thinking indicator, the system prioritizes searching for paths that connect all identified topic entities in the sequence suggested by the model. The search initiates at the predicted depth (Dpredict​) and incrementally increases towards the maximum depth (Dmax​) only if necessary. This strategy focuses computational resources on the most logically probable connections first.
    
2. LLM Supplement Path Exploration: Acknowledging that KGs may be incomplete or lack specific connections, this stage utilizes the LLM's internal parametric knowledge. The model is prompted to predict potentially missing entities or logical connections based on the current context. These predicted entities are then verified against the KG; if valid, they are incorporated into the exploration pool to uncover supplementary paths that rigid graph traversal might miss.
    
3. Node Expand Exploration: Should the initial stages fail to yield sufficient evidence, the system expands the search space by exploring the neighbors of currently unvisited entities. Unlike methods that treat entities and relations as separate search targets, PoG explores them simultaneously to maintain semantic coherence. New triples are merged into existing paths, and the search iterates until a valid path is identified or the maximum depth limit is reached.

Path Pruning. Given the sheer scale of KGs, the exploration phase can generate a substantial number of candidate paths. To strictly filter these candidates and ensure relevance, PoG employs a three-step beam search strategy during the Path Pruning phase. The process begins with Fuzzy Selection, a rapid filtering step that uses a pre-trained language model (e.g., SentenceBERT) to compute the semantic similarity between the reasoning paths and the LLM's thinking indicator. The top paths (W1​) with the highest similarity scores are retained for further analysis. These survivors then undergo Precise Path Selection, where the LLM itself evaluates the paths, scoring and selecting the top few (Wmax​) that are most likely to contain the correct answer. Finally, Branch Reduced Selection is integrated to optimize efficiency for long natural language paths. This method analyzes paths step-by-step (hop-by-hop), pruning branches that diverge from the query's intent early in the sequence. This structural pruning significantly reduces the token load on the LLM by discarding irrelevant path segments before they require full evaluation.

Question Answering. The final phase involves synthesizing the answer from the refined set of reasoning paths. To prevent the LLM from being overwhelmed by raw graph data, a Path Summarizing step first prompts the model to condense the retrieved paths into concise, relevant facts. Subsequently, the LLM evaluates whether these summarized paths provide sufficient evidence to answer both the split sub-questions and the main question. If the evidence is deemed sufficient, the model generates the final response, explicitly citing the specific path used as the reasoning basis. This ensures the output is not only accurate but also interpretable, allowing the user to trace the logic back to specific KG entries. If the evidence is insufficient, the system loops back to the exploration phase to search for deeper or alternative paths, continuing the cycle until a satisfactory answer is found or resources are exhausted.