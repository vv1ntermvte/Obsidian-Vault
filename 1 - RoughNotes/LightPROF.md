LightPROF is a framework that was recently introduced as a "Retrieve-Embed-Reason" approach to address some of the aforementioned challenges, specifically the high resource consumption of large-scale models and the loss of structural information when converting KGs to text. It is designed to empower small-scale Large Language Models (LLMs) with the ability to perform complex reasoning tasks by leveraging precise retrieval mechanisms and fine-grained structured data processing. As depicted in the framework's architecture, the process is divided into three distinct stages: Reasoning Graph Retrieval, Knowledge Embedding, and Knowledge Prompts Mixed Reasoning.

The first stage, Reasoning Graph Retrieval, is paramount for complex multi-hop Question Answering (QA) tasks, as it determines how efficiently and accurately information is sourced from the KG. To optimize this, the retrieval module is further segmented into three specific steps: semantic extraction, relation retrieval, and reasoning graph sampling. Initially, the semantic extraction step focuses on narrowing the retrieval scope while preserving essential reasoning data. For a given question, a pre-trained language model, such as BERT, is fine-tuned to analyze the semantic vector of the query. This model learns to predict the specific number of hops h_q required within the KG to adequately answer the question, effectively framing the hop-prediction as a classification task.

Following the determination of the necessary hop depth, the process moves to relation retrieval. Unlike entities, which can be complex and continuously changing, relations in KGs offer a more stable and intuitive unit for retrieval, providing clear semantic connections between entities. To maximize the gathering of relevant knowledge, the model initiates a constrained breadth-first search (BFS) starting from identified anchor entities in the question. The previously predicted number of hops h_q is now utilized as an upper bound for the depth of this search, ensuring that the system collects all relation links extending from the anchor entity up to the predetermined length without unnecessary expansion. Subsequently, in the reasoning graph sampling step, these retrieved relation links are evaluated by an LLM, which scores and ranks them based on their semantic relevance to the original question. By selecting the top-ranked links, the system samples the KG to extract specific reasoning paths, constructing a refined Reasoning Graph, denoted as G_R.

The second stage, Knowledge Embedding, addresses the limitation that natural language descriptions of KGs often contain redundancy and fail to convey inherent structural patterns essential for deep understanding. To resolve this, a compact Transformer-based Knowledge Adapter is employed to encode the textual content of the reasoning graph while simultaneously extracting its structural information. This adapter operates by decomposing the reasoning graph into individual reasoning paths, each consisting of a set of triples.

The embedding process is dual-faceted. First, to capture structural nuances, the system employs a structural embedding function that encodes the local structural information of each triple (head, relation, and tail). A linear layer then aggregates these local embeddings to form a global structural representation vector for the entire reasoning path. Simultaneously, to capture textual information, the system fuses the text-level embeddings of all entities and relations within a path. To preserve semantic integrity and minimize training complexity, a consolidation function, specifically a concatenation operation, combines these elements into a composite vector encapsulating the path's comprehensive textual data.

The core of this stage is the Knowledge Encoder, which seamlessly integrates the global structural vector and the comprehensive textual vector into a fused representation. This allows each reasoning path to be encoded into a single token, significantly enhancing the token utilization efficiency of the LLM. Because the fused vector contains both structural and textual elements, it enables the model to comprehend the complex interactions within the graph more accurately. Finally, a trainable projector maps these encoded tokens into the specific token embedding space of the target LLM. This transformation generates a "knowledge soft prompt," a sequence of input vectors fully compatible with the LLM, derived solely by tuning the parameters of the Knowledge Adapter and Projector.

The final stage, Knowledge Prompts Mixed Reasoning, utilizes these prepared embeddings to guide the LLM. Recognizing that retraining LLMs is costly and can lead to catastrophic forgetting, LightPROF freezes the parameters of the LLM during training. Instead, it employs a mixed prompt strategy. The input is structured in a chat format using so called "hard prompts", carefully designed text templates containing instructions and the question. The "knowledge soft prompt" representing the reasoning graph is inserted into a specific location within this hard prompt. This effective injection of external knowledge allows the LLM to perform autonomous and accurate reasoning based on the provided context. The training objective focuses on maximizing the likelihood of generating the correct answer sequence given the combined hard and soft prompts, ensuring the model is optimized for precise next-token prediction without altering its foundational parameters.