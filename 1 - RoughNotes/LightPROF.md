To address the aforementioned challenges—specifically the high resource consumption of large-scale models and the loss of structural information when converting Knowledge Graphs (KGs) to text—the LightPROF framework is introduced as a novel "Retrieve-Embed-Reason" approach1111. This methodology is designed to empower small-scale Large Language Models (LLMs) with the ability to perform complex reasoning tasks by leveraging precise retrieval mechanisms and fine-grained structured data processing2. As depicted in the framework's architecture, the process is divided into three distinct stages: Reasoning Graph Retrieval, Knowledge Embedding, and Knowledge Prompts Mixed Reasoning3.

The first stage, **Reasoning Graph Retrieval**, is paramount for complex multi-hop Question Answering (QA) tasks, as it determines how efficiently and accurately information is sourced from the KG4. To optimize this, the retrieval module is further segmented into three specific steps: semantic extraction, relation retrieval, and reasoning graph sampling5. Initially, the semantic extraction step focuses on narrowing the retrieval scope while preserving essential reasoning data6. For a given question, a pre-trained language model, such as BERT, is fine-tuned to analyze the semantic vector of the query7. This model learns to predict the specific number of hops ($h_q$) required within the KG to adequately answer the question, effectively framing the hop-prediction as a classification task8.

Following the determination of the necessary hop depth, the process moves to relation retrieval. Unlike entities, which can be complex and continuously changing, relations in KGs offer a more stable and intuitive unit for retrieval, providing clear semantic connections between entities9999. To maximize the gathering of relevant knowledge, the model initiates a constrained breadth-first search (BFS) starting from identified anchor entities in the question10. The depth of this search is strictly limited by the predicted hop count ($h_q$), ensuring that the system collects all relation links extending from the anchor entity up to the predetermined length without unnecessary expansion11. Subsequently, in the reasoning graph sampling step, these retrieved relation links are evaluated by an LLM, which scores and ranks them based on their semantic relevance to the original question12. By selecting the top-ranked links, the system samples the KG to extract specific reasoning paths, constructing a refined and highly relevant Reasoning Graph, denoted as $G_R$13.

The second stage, **Knowledge Embedding**, addresses the limitation that natural language descriptions of KGs often contain redundancy and fail to convey inherent structural patterns essential for deep understanding14. To resolve this, a compact Transformer-based Knowledge Adapter is employed to encode the textual content of the reasoning graph while simultaneously extracting its structural information15151515. This adapter operates by decomposing the reasoning graph into individual reasoning paths, each consisting of a set of triples16.

The embedding process is dual-faceted. First, to capture structural nuances, the system employs a structural embedding function that encodes the local structural information of each triple (head, relation, and tail)17. A linear layer then aggregates these local embeddings to form a global structural representation vector for the entire reasoning path18. Simultaneously, to capture textual information, the system fuses the text-level embeddings of all entities and relations within a path19. To preserve semantic integrity and minimize training complexity, a consolidation function—specifically a concatenation operation—combines these elements into a composite vector encapsulating the path's comprehensive textual data.

The core of this stage is the Knowledge Encoder, which seamlessly integrates the global structural vector and the comprehensive textual vector into a fused representation21. This allows each reasoning path to be encoded into a single token, significantly enhancing the token utilization efficiency of the LLM22. Because the fused vector contains both structural and textual elements, it enables the model to comprehend the complex interactions within the graph more accurately23232323. Finally, a trainable projector maps these encoded tokens into the specific token embedding space of the target LLM24. This transformation generates a "knowledge soft prompt," a sequence of input vectors fully compatible with the LLM, derived solely by tuning the parameters of the Knowledge Adapter and Projector.

The final stage, **Knowledge Prompts Mixed Reasoning**, utilizes these prepared embeddings to guide the LLM. Recognizing that retraining LLMs is costly and can lead to catastrophic forgetting, LightPROF freezes the parameters of the LLM during training26262626. Instead, it employs a mixed prompt strategy. The input is structured in a chat format using "hard prompts"—carefully designed text templates containing instructions and the question27. The "knowledge soft prompts" representing the reasoning graph are inserted into specific locations within these hard prompts28. This effective injection of external knowledge allows the frozen LLM to perform autonomous and accurate reasoning based on the provided context29. The training objective focuses on maximizing the likelihood of generating the correct answer sequence given the combined hard and soft prompts, ensuring the model is optimized for precise next-token prediction without altering its foundational parameters.